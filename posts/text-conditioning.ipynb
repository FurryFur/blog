{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Text Conditioning Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some input text to condition on, the algorithm currently produces audio samples that are very similar to each other, regardless of the input latent vector 'z'. This is likely a form a mode collapse caused by feeding the generator with data containining discontinuities (text input data is discrete). Since we have a limited, fixed set of descriptions for sound effects, using this text to create text embeddings results in a discrete set of conditioning variables. When fed into the generator, this causes the generator to to converge to a single example it can use to fool the discriminator, regardless of the input latent vector. In order to solve this, we need to feed continuous data to the generator. We can do this by sampling from a Gaussian distribution, N(u(t), sigma(t)), with u(t) and sigma(t) being functions of the original text embedding. Note that at generation time, we should use the provided text embedding directly, without re-sampling from the distribution. Since we don't know exactly what a good value or function for u(t) and sigma(t) would be, we can simple have the computer learn these functions by using a fully connected dense layer to output u(t) and sigma(t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Code from StackGAN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g-net\n",
    "def generate_condition(self, c_var):\n",
    "    conditions =\\\n",
    "        (pt.wrap(c_var).\n",
    "         flatten().\n",
    "         custom_fully_connected(self.ef_dim * 2).\n",
    "         apply(leaky_rectify, leakiness=0.2))\n",
    "    mean = conditions[:, :self.ef_dim]\n",
    "    log_sigma = conditions[:, self.ef_dim:]\n",
    "    return [mean, log_sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_encoded_context(self, embeddings):\n",
    "        '''Helper function for init_opt'''\n",
    "        c_mean_logsigma = self.model.generate_condition(embeddings)\n",
    "        mean = c_mean_logsigma[0]\n",
    "        if cfg.TRAIN.COND_AUGMENTATION:\n",
    "            # epsilon = tf.random_normal(tf.shape(mean))\n",
    "            epsilon = tf.truncated_normal(tf.shape(mean))\n",
    "            stddev = tf.exp(c_mean_logsigma[1])\n",
    "            c = mean + stddev * epsilon\n",
    "\n",
    "            kl_loss = KL_loss(c_mean_logsigma[0], c_mean_logsigma[1])\n",
    "        else:\n",
    "            c = mean\n",
    "            kl_loss = 0\n",
    "\n",
    "        return c, cfg.TRAIN.COEFF.KL * kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nikola": {
   "category": "",
   "date": "2018-09-22 10:16:30 UTC+12:00",
   "description": "",
   "link": "",
   "slug": "text-conditioning",
   "tags": "",
   "title": "Text Conditioning",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
