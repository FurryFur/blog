{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Text Conditioning Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given some input text to condition on, the algorithm currently produces audio samples that are very similar to each other, regardless of the input latent vector 'z'. This is likely a form a mode collapse caused by feeding the generator with data containining discontinuities (text input data is discrete). Since we have a limited, fixed set of descriptions for sound effects, using this text to create text embeddings results in a discrete set of conditioning variables. When fed into the generator, this causes the generator to to converge to a single example it can use to fool the discriminator, regardless of the input latent vector. In order to solve this, we need to feed continuous data to the generator. We can do this by sampling from a Gaussian distribution, N(u(t), sigma(t)), with u(t) and sigma(t) being functions of the original text embedding. Note that at generation time, we should use the provided text embedding directly, without re-sampling from the distribution. Since we don't know exactly what a good value or function for u(t) and sigma(t) would be, we can simple have the computer learn these functions by using a fully connected dense layer to output u(t) and sigma(t)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Code from StackGAN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g-net\n",
    "def generate_condition(self, c_var):\n",
    "    conditions =\\\n",
    "        (pt.wrap(c_var).\n",
    "         flatten().\n",
    "         custom_fully_connected(self.ef_dim * 2).\n",
    "         apply(leaky_rectify, leakiness=0.2))\n",
    "    mean = conditions[:, :self.ef_dim]\n",
    "    log_sigma = conditions[:, self.ef_dim:]\n",
    "    return [mean, log_sigma]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_encoded_context(self, embeddings):\n",
    "        '''Helper function for init_opt'''\n",
    "        c_mean_logsigma = self.model.generate_condition(embeddings)\n",
    "        mean = c_mean_logsigma[0]\n",
    "        if cfg.TRAIN.COND_AUGMENTATION:\n",
    "            # epsilon = tf.random_normal(tf.shape(mean))\n",
    "            epsilon = tf.truncated_normal(tf.shape(mean))\n",
    "            stddev = tf.exp(c_mean_logsigma[1])\n",
    "            c = mean + stddev * epsilon\n",
    "\n",
    "            kl_loss = KL_loss(c_mean_logsigma[0], c_mean_logsigma[1])\n",
    "        else:\n",
    "            c = mean\n",
    "            kl_loss = 0\n",
    "\n",
    "        return c, cfg.TRAIN.COEFF.KL * kl_loss\n",
    "    \n",
    "# cfg.TRAIN.COEFF.KL = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce_mean normalize also the dimension of the embeddings\n",
    "def KL_loss(mu, log_sigma):\n",
    "    with tf.name_scope(\"KL_divergence\"):\n",
    "        loss = -log_sigma + .5 * (-1 + tf.exp(2. * log_sigma) + tf.square(mu))\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the resampling to the text embedding we start to get some decent output after 10000 steps. However, the generator is still ignoring the latent 'z' vector, in favor of generating all its output directly from the input context. Part of the problem is that currently the text embedding occupies over 2/3rds of the generator's input space (z vector dims = 100, text embedding dims = 256). Since the text embedding is also highly correlated with the output that the discriminator expects, the generator tends to rely heavily on the text embedding to produce its output. This causes the generator to learn to ignore 'z', as using 'z' to create variety doesn't help the generator fool the discriminator. In order to mitigate th over reliance on the text embedding, dropout is added to the layer that produces the embedding in both the discriminator and the generator. The size of the text embedding is also reduced to 128 to make it more balanced with the latent 'z' vector. \n",
    "\n",
    "Another approach to mitigating this issue would be to save the text embeddings, and re-sampled embeddings, for previous runs of training. Then, occasionally replay these during training. It is important to ensure that the generator uses the saved re-sampled embedding as is, without trying to re-sample it again. This will ensure that the generator would need to rely on the latent 'z' vector (which is not saved), in order to fool the discriminator a second time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nikola": {
   "category": "",
   "date": "2018-09-22 10:16:30 UTC+12:00",
   "description": "",
   "link": "",
   "slug": "text-conditioning",
   "tags": "",
   "title": "Text Conditioning",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
