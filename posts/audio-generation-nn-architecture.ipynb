{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Architecture Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent vector, 'Z', would be better drawn from a standard normal distribution, rather than uniformly. It is reasonable to assume that some population would be normally distributed, as it is the most common distribution in nature. No natural population is ever uniformly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different activations functions used in the generator have a distinct effect on the audio produced. A tanh activation function generally produces a distinct warbleing and fluttering sound into the generated audio. In general tanh seems to converge to useful samples much easier than relu. This better audio quality of tanh is also noted in the wavenet paper by google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the *structured* noise (especially in the initial samples) in the audio samples seems to come from the up-sampling method used to generate the higher dimensional audio output, from a lower dimensional latent space. It has previously been noted that using the standard transpose convolution operation results in inherent artifacts in the output (https://distill.pub/2016/deconv-checkerboard/). In contrast, using a nearest neighbour upsample + convolution (nn-conv) produces none of the artifacts present in the transpose convolution method. The nn-conv method seems to represent a much better alternative then to the transpose convolution. However, in practice, I found that the nn-conv method had a much harder job converging to useful samples than the standard transpose convolution. The authors of the original WaveGAN paper noted these up-sampling artifacts as well, however, they suggested that these artifiacts might be necessary for creating fine grained detail in the resulting audio. In fact they even actively discourage the discriminator from finding these regular artifacts by randomly shifting the activations of each layer in the discriminator. This is an operation they refer to as 'phase shuffling'. Unfotunately the authors of the WaveGAN paper did not go into much detail regarding other upsampling methods, only noting that they experimented with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "nikola": {
   "category": "",
   "date": "2018-09-22 10:33:57 UTC+12:00",
   "description": "",
   "link": "",
   "slug": "audio-generation-nn-architecture",
   "tags": "",
   "title": "Audio Generation NN Architecture",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
